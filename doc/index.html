<html>

<head>
<title>Crawl - The Java Edition</title>
<link rel="stylesheet" href="http://www.genedb.org/includes/style/genedb/main.css" type="text/css" />

 <script type="text/javascript" src="http://www.genedb.org/includes/scripts/jquery/jquery-genePage-combined-new.js"></script>
<script src="http://samaxesjs.googlecode.com/files/jquery.toc-1.1.1.js"></script>

<style>

body {
    font-size: 1em;
    width:900px;
    margin:auto;
}

h1 { font-size:2em; }
h2 { font-size:1.8em; }
h3 { font-size:1.6em; }
h4 { font-size:1.4em; }
h5 { font-size:1.2em; }

h1,h2,h3,h4,h5 {
    font-weight:bold;
    text-transform: uppercase;
}

p {
    padding:15px;
    text-align: justify;
}

pre {
    margin:10px;
    padding:10px;
    border: 1px dashed grey;
}

table {
    margin:20px;
    
}

table, td, th {
    border: 1px dashed grey;
}

td, th {
    padding:10px;
}

div#toc {
    border: 1px dashed grey;
    padding:10px;
    width:400px;
    margin:auto;
}

div#toc ul {
    list-style: none;
    text-transform: uppercase;
    font-size:small;
}
div#toc ul li ul {
    margin-bottom: 0.75em;
}
div#toc ul li ul li ul {
    margin-bottom: 0.25em;
}

a, a:visited {
text-decoration:none;
color:grey;
}

a:hover {
color:black;
}

</style>
</head>
<body>



<h1>Crawl - The Java Edition</h1>


<div id="toc"></div>
<P>
<center>
    <em>
        Note: Generated using <a href="http://jquery.org/">Jquery</a> and a <a href="http://code.google.com/p/samaxesjs/wiki/TableOfContentsPlugin">TOC plugin</a>.
    </em>
</center>
</P>


<h2 id="Intro">Introduction</h2>

<P>Crawl provides a standardised API, in the form of web services, for displaying genomic data. It can work directly off 
a <a href="http://gmod.org/wiki/Chado">Chado</a> database, 
or instead off an <a href="http://www.elasticsearch.org">ElasticSearch</a> cluster, which 
uses <a href="http://lucene.apache.org/">Lucene</a> as a back end, and can be indexed off a combination of data sources.</P>

<P>It is currently used by <a href="https://github.com/sanger-pathogens/Web-Artemis">Web-Artemis</a> as an AJAX back end.</P>

<h3 id="Chado">Chado</h3>

<P>Chado is a standard relational database, typically deployed on <a href="http://www.postgresql.org">postgres</a> database. It has a very flexible schema, allowing all sorts of genomic
features to be modelled. It is the underlying data store for <a href="http://www.genedb.org">GeneDB</a>, <a href="http://flybase.org/">FlyBase</a>, 
<a href="http://paramecium.cgm.cnrs-gif.fr/">ParameciumDB</a>, <a href="http://www.vectorbase.org/">VectorBase</a>, and <a href="http://www.gnpannot.org/">GNPAnnot</a>. 
</P>

<P>The Crawl API exposes the contents of these databases via SQL queries.</P>

<h3 id="ElasticSearch">ElasticSearch</h3>

<P>Data can be indexed and stored using ElasticSearch, a distributed search enging that runs ontop of Lucene. There are two kinds of connections to ElasticSearch: local and transport. Local connections
 are useful to get going with, as an ElasticSearch cluster is automatically created and managed in the same process as Crawl. The only configuration needed for this is usually the location of the 
 indices. Transport connections connect to a separately instantiated ElasticSearch cluster, which can be on the same or remote machines, and the configuration needed for this is host and port.</P> 
 
<P>The indices can be populated from various data sources: GFF3, DAS and Chado. It might seem odd to want to index a Chado database when Crawl will run directly off of it, but this might be 
useful in situations where you want to merge annotations from Chado with other sources (like DAS or GFF3).</P>


<P>The Crawl API exposes the contents of these indices via ElasticSearch (Lucene) queries.</P>

<h3 id="align">Alignments</h3>

<P>Crawl can retrieve segments from Next-Generation sequencing alignments stored in BAM files, for overlaying ontop of genomic features.</P>


<h2 id="build">Setup, build and test</h2>

<p>This is all done through the use of <a href="http://gradle.org/">Gradle</a>. You don't need to install it, though, because there will be an executable in the root folder of the checkout
('gradlew').</p>

<h3 id="setup">Setup</h3>

<P>The initial build setup might take a while because it will download dependencies, but you should only ever have to run it 
all once. After that any updates to dependencies should be incremental.To setup the build, run : </P>

<pre><code>./gradlew</code></pre>

<P>in the crawl checkout root folder. This can take 5-10 minutes depending on your internet connection.</P>

<h3 id="dependencies">Fetching dependencies from the GeneDB repository (optional)</h3>

<P>Because of the unreliability of some external repositories, the dependencies are hosted on the genedb developer site as well. If you find the resolve step fails, you can try
you can enable download from this site by adding the genedb developer https certificate. This certificate can be downloaded like this on a Mac (in bash) : </P>

<pre><code>openssl s_client -connect developer.genedb.org:443 2>&1 | \
    sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' \
    > /tmp/developer.genedb.org
</code></pre>

<P>And imported as follows :</P>

<pre><code>sudo keytool -import -alias developer.genedb.org \
    -keystore /Library/Java/Home/lib/security/cacerts \
    -file /tmp/developer.genedb.org 
</code></pre>

<P>The keytool password is 'changeit'.</P>

<P>Finally, you will have to uncomment the repositories in the build.gradle file that have a url that starts with :</P>

<pre><code>https://developer.genedb.org/nexus/content/repositories/</code></pre>

<h3 id="buildandtest">Build and test</h3>

<P>Now to build, run :</P>

<pre><code>./gradlew build</code></pre>


<P>This will perform all the build steps, as well as run a test harness. You shouldn't worry about configurations for the test harness, because the default configurations
should just work out of the box. The reason for that will be explained later, but the gist of it is that it connects to the GeneDB public snapshot for its database tests,
and creates a local ElasticSearch cluster for its indexing tests.</P>


<h3 id="eclipse">Customizing eclipse</h3>

<P>If you want to be able to edit the code inside eclipse, you can do. However, you need to comment out this line in the build.gradle file :</P>

<pre><code>compile fileTree(dir: 'lib/local', includes: ['**.jar'])</code></pre>

<P>before running</P>

<pre><code>./gradlew eclipse</code></pre>

<P><strong>Remember to then uncomment that line again before running any other gradle tasks. </strong>This is due to a currently open issue with 
gradle <a href="http://issues.gradle.org/browse/GRADLE-1415">GRADLE-1415</a>.</P>

<h2 id="indexing">Indexing</h2>

<p>These are done using shell scripts. If using a stand-alone ElasticSearch cluster, then it should have been started prior to running these steps. The examples 
here use a local cluster, which is instantiated by crawl itself. All examples here use bash.</p> 

<h3 id="create organism">Creating an organism manually</h3>

<P>An organism is created by supplying its json using the -o option : 

<pre><code>./sh/gff2es.sh -pe resource-elasticsearch-local.properties -o '{
    "ID":27,
    "common_name":"Pfalciparum",
    "genus":"Plasmodium",
    "species":"falciparum",
    "translation_table":11,
    "taxonID":5833
}'</code></pre>


<h3 id="copy organism">Copying organisms from a Chado instance</h3>

<P>As with all examples of copying data from Chado, two properties files are specified: one for Chado connection details, and the other for ElasticSearch.
This example connects to the GeneDB public Chado snapshot. To just copy one organism entry, you can do :</P>

<pre><code>./sh/chado2es.sh -pc resource-chado-public.properties \
    -pe resource-elasticsearch-local.properties \
    -o Tcruzi</code></pre>

<P>To copy all of them :</P>

<pre><code>./sh/chado2es.sh -pc resource-chado-public.properties \
    -pe resource-elasticsearch-local.properties </code></pre>



<h3 id="gffs">Indexing GFF files</h3>

<p>GFF files do not contain information about the organism, so this must be supplied. This can be done as follows :</p>

<pre><code>./sh/gff2es.sh -g /path/to/gff/files \
    -o '{"common_name":"Pfalciparum"}' \
    -pe resource-elasticsearch-local.properties </code></pre>

<p>You only need to supply a unique identifier for the organism at this stage, which is either its ID or common name.</p>

<P>The -g flag (gffs) is used to specify a path to a GFF file or a folder containing them. If they are gzipped, the files will automatically be unzipped by the parser.</P>



<h3 id="incrementals">Incrementally indexing Chado</h3>

<P>As indexing from Chado to ElasticSearch requires both kinds of connection details, you will need to specify two property files.</P>

<p>Incremental indexing relies on the use of the timelastmodified stamp. You can indexing everything that has changed since 2011-01-05, as follows :</p>

<pre><code>./sh/chado2es.sh -pc resource-chado-public.properties \
    -pe resource-elasticsearch-local.properties -s 2011-04-01</code></pre>

<p>You can filter on organism:</p>

<pre><code>./sh/chado2es.sh -pc resource-chado-public.properties \
    -pe resource-elasticsearch-local.properties -o Lmajor -s 2011-04-01 </code></pre>

<p>And if you want to index the entire organism, currently the way to do it is to choose a suitably distant since date, for example :</p>

<pre><code>./sh/chado2es.sh -pc resource-chado-public.properties \
    -pe resource-elasticsearch-local.properties -o Lmajor -s 2001-01-01 </code></pre>

<h3  id="indexing regions">Indexing regions from Chado</h3>

<pre><code>./sh/chado2es.sh -pc resource-chado-public.properties \
    -pe resource-elasticsearch-local.properties \
    -r Pf3D7_01</code></pre>


<h2  id="alignments">Alignments</h2>

<P>Pointing to the alignments is fairly straightforward - an example configuration file is provided in etc/alignments.json. </P>

<h3  id="renaming sequences">Renaming sequences</h3>

<P>The only thing to be aware of is that sometimes 
the FASTA sequence names in the alignments file may not correspond to the sequences in the annotation repository, in which case it's important to rename them, and reindex
the BAM file. Crawl provides a utility for renaming sequences. In the example below, the Pfaciparum chromosomes are known ad MALX (where X is the chromosome number) in the BAM file, but the annotation
repository knows them as Pf3D7_XX.</P>  

<pre><code># convert bam sequences to the chromosome names used in the repository, using the tool provided.
./sh/bam2bam.sh -i /path/to/alignments/Tophat.MAL_Version3.dir.1788_1.bam \
    -o ~/tmp/out.bam \
    -m MAL1=Pf3D_01 -m MAL2=Pf3D7_02 -m MAL3=Pf3D7_03 \
    -m MAL4=Pf3D7_04 -m MAL5=Pf3D7_05 

# overwrite the original BAM
cp /tmp/out.bam /path/to/alignments/3D7_Version7_Tophat_RNASeq/Tophat.MAL_Version3.dir.1788_1.bam

# reindex the new BAM with samtools
samtools index /path/to/alignments/3D7_Version7_Tophat_RNASeq/Tophat.MAL_Version3.dir.1788_1.bam</code></pre>  


<h2  id="deploying">Deploying</h2>

<h3  id="war" >Packaging the war </h3>

<p>Here you must specify a config property file. Several of examples are bundled in the top level folder. For example :</p>
<pre><code>./gradlew -Pconfig=resource-elasticsearch-local.properties war</code></pre>

<h3  id="jetty">Testing the war using Jetty-runner </h3>

<P>This can be used to run crawler without having to configure a servlet container. This is not good for production use. Only use it for testing.</P>

<pre><code>./gradlew -Pconfig=resource-elasticsearch-local.properties jettyRunWar</code></pre>

<h3 id="j2ee">Deploying the war using to a J2EE container  </h3>

<P>For production, or even beta test sites, it should be deployed to a container as follows.</P>

<pre><code>./gradlew -Pconfig=resource-elasticsearch-local.properties deploy</code></pre>



<h2  id="cleaning">Cleaning up</h2>

<h3 id="clean build">Cleaning up builds</h3>

<P>To be run whenever method signatures change and you want to spot external references to them.</P> 

<pre><code>./gradlew clean</code></pre>


<h3  id="clean es">Cleaning up a local ElasticSearch data</h3>

<P>This task is only useful for non-trasnsport, i.e. local, clusters, and requires the correct config : </P> 

<pre><code>./gradlew -Pconfig=resource-elasticsearch-local.properties cleanes</code></pre>















<h2  id="props">Property files</h2>



<p>The purpose of these is to specify environmental variables used by crawl. Sometimes only one is used, as is the case when building a war. Sometimes two are used, as is the case when indexing from one data source to another. The following table describes what they are for :</p>

<table >
<tr><th>Property</th><th>Description</th><th>Configurations</th></tr>

<tr><td>resource.type</td>
<td>Currently be either chado-postgres, elasticsearch-local, elasticsearch-remote</td>
<td>all</td></tr>

<tr><td>deploy.dir</td>
<td>The tomcat webapps folder to which any war that is built using this file is deployed to.</td>
<td>all</td></tr>

<tr><td>deploy.name</td>
<td>what the war will be called, which will reflected in the URL domain path.</td>
<td>all</td></tr>

<tr><td>alignments</td>
<td>the path to an alignments.json file. This is where BAM alignments are specified. An example alignments.json file is included in the etc folder.</td>
<td>all</td></tr>

<tr><td>showParameters</td>
<td>whether or not to always return request parameters in the reponse.</td>
<td>all</td></tr>




<tr><td>dbhost</td>
<td>the chado database host</td>
<td>chado-postgres</td></tr>

<tr><td>dbport</td>
<td>the chado database port</td>
<td>chado-postgres</td></tr>

<tr><td>dbname</td>
<td>the chado database name</td>
<td>chado-postgres</td></tr>

<tr><td>dbuser</td>
<td>the chado database user</td>
<td>chado-postgres</td></tr>

<tr><td>dbpassword</td>
<td>the chado database password</td>
<td>chado-postgres</td></tr>


<tr><td>resource.elasticsearch.index</td>
<td>The name of the index</td>
<td>all ElasticSearch configurations</td></tr>

<tr><td>resource.elasticsearch.regionType</td>
<td>The name of the type representing regions</td>
<td>all ElasticSearch configurations</td></tr>

<tr><td>resource.elasticsearch.featureType</td>
<td>The name of the type representing features</td>
<td>all ElasticSearch configurations</td></tr>

<tr><td>resource.elasticsearch.organismType</td>
<td>The name of the type representing organisms</td>
<td>all ElasticSearch configurations</td></tr>




<tr><td>resource.elasticsearch.address.host</td>
<td>the elastic search host </td>
<td>elasticsearch-remote</td></tr>
<tr><td>resource.elasticsearch.address.port</td>
<td>the elastic search port </td>
<td>elasticsearch-remote</td></tr>
<tr><td>resource.elasticsearch.local.pathlogs</td>
<td>the elastic search log path </td>
<td>elasticsearch-local</td></tr>
<tr><td>resource.elasticsearch.local.pathdata</td>
<td>the elastic search data path </td>
<td>elasticsearch-local</td></tr>




</table>


<p>Not all of these parameters are used in all property files, the third column specifies in what context they are applicable.</p>

<script>

$(document).ready(function() {
    $('#toc').toc();
});

</script>

</body>

</html>